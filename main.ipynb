{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR = 0.2 # Dropout rate\n",
    "EDL = 5  # Encoder-decoder layers\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_vocab_size,word_emb_size ,out_vocab_size, heads,hidden_dim,layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.in_vocab_size = in_vocab_size\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.positional_encoder = PositionalEncoding(word_emb_size).to(device)\n",
    "        \n",
    "        encoder_layers = TransformerEncoderLayer(in_vocab_size, 1, hidden_dim, DR)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, layers)\n",
    "        self.encoder = nn.Embedding(word_emb_size, word_emb_size)\n",
    "        self.word_embeds = nn.Embedding(in_vocab_size, word_emb_size)\n",
    "        self.word_emb_size = word_emb_size\n",
    "        self.decoder = nn.Linear(word_emb_size, out_vocab_size)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        src = self.encoder(src) * math.sqrt(self.in_vocab_size)\n",
    "        src = self.positional_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        self.activation = nn.Sigmoid()\n",
    "     \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.encoder.weight)\n",
    "        nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output =  self.activation(self.decoder(output))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import spacy\n",
    "tknzr = TweetTokenizer()\n",
    "import os \n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split('\\n')\n",
    "\n",
    "\n",
    "english_sentences = load_data('corpus.en_ru.1m.en')\n",
    "russian_sentences = load_data('corpus.en_ru.1m.ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\arsee\\Anaconda3\\envs\\nn_in_arts\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\arsee\\Anaconda3\\envs\\nn_in_arts\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\arsee\\Anaconda3\\envs\\nn_in_arts\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\arsee\\Anaconda3\\envs\\nn_in_arts\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\arsee\\Anaconda3\\envs\\nn_in_arts\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\arsee\\Anaconda3\\envs\\nn_in_arts\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False,num_words=40000)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "\n",
    "def preprocess(x, y):\n",
    "    \n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    m = max(max([len(sentence) for sentence in preprocess_x]),max([len(sentence) for sentence in preprocess_y]))\n",
    "    preprocess_x = pad(preprocess_x,m)\n",
    "    preprocess_y = pad(preprocess_y, m)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_russian_sentences, preproc_english_sentences, english_tokenizer, russian_tokenizer =\\\n",
    "    preprocess(russian_sentences,english_sentences)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_russian_sequence_length = preproc_russian_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "russian_vocab_size = len(russian_tokenizer.word_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 196])\n",
      "torch.Size([1000000, 196])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "en_data = torch.from_numpy(preproc_english_sentences).long().to(device)\n",
    "ru_data = torch.from_numpy(preproc_russian_sentences).long().to(device)\n",
    "print(en_data.shape)\n",
    "print(ru_data.shape)\n",
    "\n",
    "dataset = data.TensorDataset(ru_data,en_data) \n",
    "dataloader = data.DataLoader(dataset,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ntokens =english_vocab_size # the size of vocabulary\n",
    "emsize = 32  # embedding dimension\n",
    "nhid = 32 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 3 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "cnt =0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cur loss: 5.23063588142395 Epoch: 0 %: 0.006\n",
      "Cur loss: 5.192591698964437 Epoch: 0 %: 0.012\n",
      "Cur loss: 5.196296755472819 Epoch: 0 %: 0.018000000000000002\n",
      "Cur loss: 5.190289640426636 Epoch: 0 %: 0.024\n",
      "Cur loss: 5.199387168884277 Epoch: 0 %: 0.03\n",
      "Cur loss: 5.1892349084218345 Epoch: 0 %: 0.036000000000000004\n",
      "Cur loss: 5.178488938013713 Epoch: 0 %: 0.042\n",
      "Cur loss: 5.177216831843058 Epoch: 0 %: 0.048\n",
      "Cur loss: 5.195608838399251 Epoch: 0 %: 0.054\n",
      "Cur loss: 5.1824645360310875 Epoch: 0 %: 0.06\n",
      "Cur loss: 5.173548984527588 Epoch: 0 %: 0.066\n",
      "Cur loss: 5.1875540574391685 Epoch: 0 %: 0.07200000000000001\n",
      "Cur loss: 5.183758052190145 Epoch: 0 %: 0.078\n",
      "Cur loss: 5.191748762130738 Epoch: 0 %: 0.084\n",
      "Cur loss: 5.190353377660116 Epoch: 0 %: 0.09\n",
      "Cur loss: 5.198275232315064 Epoch: 0 %: 0.096\n",
      "Cur loss: 5.193278773625692 Epoch: 0 %: 0.10200000000000001\n",
      "Cur loss: 5.175868066151937 Epoch: 0 %: 0.108\n",
      "Cur loss: 5.181651465098063 Epoch: 0 %: 0.11399999999999999\n",
      "Cur loss: 5.181227016448974 Epoch: 0 %: 0.12\n",
      "Cur loss: 5.195595216751099 Epoch: 0 %: 0.126\n",
      "Cur loss: 5.173082733154297 Epoch: 0 %: 0.132\n",
      "Cur loss: 5.186890061696371 Epoch: 0 %: 0.13799999999999998\n",
      "Cur loss: 5.184119860331218 Epoch: 0 %: 0.14400000000000002\n",
      "Cur loss: 5.189554850260417 Epoch: 0 %: 0.15\n",
      "Cur loss: 5.186465644836426 Epoch: 0 %: 0.156\n",
      "Cur loss: 5.164008776346843 Epoch: 0 %: 0.16199999999999998\n",
      "Cur loss: 5.188509543736775 Epoch: 0 %: 0.168\n",
      "Cur loss: 5.175505574544271 Epoch: 0 %: 0.174\n",
      "Cur loss: 5.154905128479004 Epoch: 0 %: 0.18\n",
      "Cur loss: 5.153998247782389 Epoch: 0 %: 0.186\n",
      "Cur loss: 5.164451662699381 Epoch: 0 %: 0.192\n",
      "Cur loss: 5.176938072840373 Epoch: 0 %: 0.198\n",
      "Cur loss: 5.175103346506755 Epoch: 0 %: 0.20400000000000001\n",
      "Cur loss: 5.187129211425781 Epoch: 0 %: 0.21\n",
      "Cur loss: 5.176760466893514 Epoch: 0 %: 0.216\n",
      "Cur loss: 5.174032719930013 Epoch: 0 %: 0.22200000000000003\n",
      "Cur loss: 5.170605738957723 Epoch: 0 %: 0.22799999999999998\n",
      "Cur loss: 5.157613531748454 Epoch: 0 %: 0.234\n",
      "Cur loss: 5.1856754144032795 Epoch: 0 %: 0.24\n",
      "Cur loss: 5.188083823521932 Epoch: 0 %: 0.246\n",
      "Cur loss: 5.178773895899455 Epoch: 0 %: 0.252\n",
      "Cur loss: 5.179831584294637 Epoch: 0 %: 0.258\n",
      "Cur loss: 5.175108035405477 Epoch: 0 %: 0.264\n",
      "Cur loss: 5.175207567214966 Epoch: 0 %: 0.27\n",
      "Cur loss: 5.181835571924846 Epoch: 0 %: 0.27599999999999997\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.01 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "for epoch in range(1):\n",
    "    model.train() \n",
    "    total_loss = 0.\n",
    "    cnt = 0\n",
    "    for (i,tar) in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inp = i\n",
    "        predicted = model(inp)\n",
    "        cnt+=1\n",
    "        \n",
    "        loss = criterion(predicted.view(-1,english_vocab_size),tar.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        log_interval = 30\n",
    "        if cnt % log_interval == 0 and cnt > 0:\n",
    "            torch.save((english_tokenizer, russian_tokenizer, model.state_dict()), \"model\")\n",
    "            cur_loss = total_loss / log_interval\n",
    "          \n",
    "            \n",
    "            print(\"Cur loss:\", cur_loss, \"Epoch:\", epoch, \"%:\", cnt/len(dataloader)*100 )\n",
    "            total_loss = 0\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
